{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "546ba4d1",
   "metadata": {},
   "source": [
    "### Task 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691fb50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the important libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mlxtend.preprocessing\n",
    "import mlxtend.frequent_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc89db8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "data_ass = pd.read_csv('assignment_basket.csv')\n",
    "\n",
    "data_ass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd75101e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequent items\n",
    "data_ass.Item.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7755ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating transactional format for afternoon\n",
    "\n",
    "data = data_ass[data_ass.period_day == 'afternoon']\n",
    "\n",
    "grocery_list = data.groupby(['Transaction'])['Item'].apply(list).values.tolist()\n",
    "\n",
    "encoder = mlxtend.preprocessing.TransactionEncoder().fit(grocery_list)\n",
    "\n",
    "encoded_data = encoder.transform(grocery_list)\n",
    "\n",
    "grocery_trans = pd.DataFrame(encoded_data, columns = encoder.columns_)\n",
    "\n",
    "# Creating transactional format for morning\n",
    "\n",
    "data_1 = data_ass[data_ass.period_day == 'morning']\n",
    "\n",
    "grocery_list_1 = data_1.groupby(['Transaction'])['Item'].apply(list).values.tolist()\n",
    "\n",
    "encoder_1 = mlxtend.preprocessing.TransactionEncoder().fit(grocery_list_1)\n",
    "\n",
    "encoded_data_1 = encoder_1.transform(grocery_list_1)\n",
    "\n",
    "grocery_trans_1 = pd.DataFrame(encoded_data_1, columns = encoder_1.columns_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b3f83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequent products in afternoon\n",
    "print('Afternoon')\n",
    "print(grocery_trans.sum().sort_values(ascending = False)[:10])\n",
    "\n",
    "# Most frequent products in morning\n",
    "print('Morning')\n",
    "print(grocery_trans_1.sum().sort_values(ascending = False)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa6ae84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Itemsets for afternoon\n",
    "frequent_itemsets = mlxtend.frequent_patterns.apriori(grocery_trans, min_support = 0.001, max_len = 4, use_colnames = True)\n",
    "frequent_itemsets.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a1d5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Itemsets for morning\n",
    "frequent_itemsets_1 = mlxtend.frequent_patterns.apriori(grocery_trans_1, min_support = 0.001, max_len = 4, use_colnames = True)\n",
    "frequent_itemsets_1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7896cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rules for afternoon\n",
    "rules = mlxtend.frequent_patterns.association_rules(frequent_itemsets, metric = \"confidence\", min_threshold = 0.6)\n",
    "rules.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ade63a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rules for morning\n",
    "rules_1 = mlxtend.frequent_patterns.association_rules(frequent_itemsets_1, metric = \"confidence\", min_threshold = 0.6)\n",
    "rules_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45eccc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Egg in the morning: Bread\n",
    "selection_1 = rules_1['antecedents'].apply(lambda x: 'Eggs' in x)\n",
    "print(rules_1[selection_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe980720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coke and Juice in the afternoon: Sandwich\n",
    "selection_2 = rules['antecedents'].apply(lambda x: 'Coke' in x and 'Juice' in x)\n",
    "print(rules[selection_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c2bd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toast in the morning or in the morning: Coffee\n",
    "selection = rules['antecedents'].apply(lambda x: 'Toast' in x)\n",
    "selection_1 = rules_1['antecedents'].apply(lambda x: 'Toast' in x)\n",
    "print(rules[selection])\n",
    "print(rules_1[selection_1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95d35c5",
   "metadata": {},
   "source": [
    "## Task 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b6f9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries to be used\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Decision trees\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error as MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7c70e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data \n",
    "heart = pd.read_csv('patients.csv')\n",
    "heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c839405e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data info\n",
    "# Seems no missing values, only numerical variables (althoguh there are somewith only two possible values)\n",
    "heart.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1499341",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# some basic statistics\n",
    "heart.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210bc628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not much difference in the number of patients in the two classes\n",
    "heart.outcome.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dee0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation\n",
    "heart.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4379deac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots are useful\n",
    "# For example, heart rate is clearly higher for class 1\n",
    "sns.boxplot(x = 'outcome', y = 'heart_rate', data = heart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52d6a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countplot for 0-1 variables\n",
    "# For example, there is not much exercise in class 1\n",
    "sns.countplot(x = \"outcome\", data = heart, hue = \"exercise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22af8b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets select the columns of interes\n",
    "# Predictors\n",
    "X = heart[heart.columns[:-1]]\n",
    "# Outcome\n",
    "y = heart['outcome']\n",
    "\n",
    "# Then we crate training and test set, with 25% of the data in the test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3686f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can start with a single decision tree model\n",
    "\n",
    "# Inititalize model, at least we set enough iterations\n",
    "cancer_lr = LogisticRegression(max_iter = 1000)\n",
    "\n",
    "# Use training data\n",
    "lr_model = cancer_lr.fit(X_train,y_train)\n",
    "\n",
    "# Evaluate perfomance\n",
    "# As we can see, we can get quite a good result already\n",
    "\n",
    "pred_lr = lr_model.predict(X_test)\n",
    "print(confusion_matrix(y_test,pred_lr))\n",
    "print(classification_report(y_test,pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3382ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by initializing the LR model, and optimize parameters\n",
    "model = LogisticRegression()\n",
    "\n",
    "# We can specify possible values for the number of iterations\n",
    "iterations = [500, 600, 700, 800]\n",
    "\n",
    "# We can try different C values\n",
    "c_values = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "# Class weights\n",
    "weights = ['balanced', {0:0.3, 1:0.7}]\n",
    "\n",
    "# We define the grid as a dictionary, using the name of parameters as defined in LogistiRegression as keys\n",
    "# We will have 4x5x2=40 possible combinations, i.e. 40 different models will be tested\n",
    "\n",
    "grid = dict(max_iter = iterations, C = c_values, class_weight = weights)\n",
    "\n",
    "# We specify the grid search\n",
    "# Estimator is the initial model, param_grid is the dictionary specified above\n",
    "# We can also specify what performance measure we want to optimize\n",
    "# We can try with recall\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, scoring='accuracy')\n",
    "\n",
    "# We fit the training data\n",
    "\n",
    "grid_result = grid_search.fit(X, y)\n",
    "\n",
    "# Print out the best results\n",
    "# It tells us what parameters ween need to chose to obtain the model with the best possible recall\n",
    "# You can try with different settings and change also scoring to, e.g. accuracy\n",
    "\n",
    "print(\"Best result is\", grid_result.best_score_, 'using', grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d05f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try decision trees\n",
    "\n",
    "heart_tree = DecisionTreeClassifier(random_state = 42)\n",
    "\n",
    "# This case we can specify possible values for \n",
    "# optimality criterion\n",
    "criterion = ['gini', 'entropy']\n",
    "\n",
    "# Maximum depth of the tree\n",
    "max_depth = [2,4,6,8,10,12]\n",
    "\n",
    "# Class weights\n",
    "weights = ['balanced', {0:0.1, 1:0.9}]\n",
    "\n",
    "# We define the grid, 24 possible models\n",
    "grid = dict(criterion = criterion, max_depth = max_depth, class_weight = weights)\n",
    "\n",
    "# We specify the grid search\n",
    "\n",
    "grid_search = GridSearchCV(estimator=heart_tree, param_grid=grid, scoring='accuracy')\n",
    "\n",
    "grid_result = grid_search.fit(X, y)\n",
    "\n",
    "# Print out the best result\n",
    "print(\"Best result is\", grid_result.best_score_, 'using', grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae677388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by creating a decision tree palceholder\n",
    "\n",
    "cancer_bag = DecisionTreeClassifier(random_state = 42)\n",
    "\n",
    "# We can then create a bagging classifier object, specify the base model, \n",
    "# and that we want to build 300 different decision trees\n",
    "# Different in this case means that they will use different subsets of the training data\n",
    "\n",
    "bag_cancer = BaggingClassifier(base_estimator = cancer_bag, n_estimators = 300)\n",
    "\n",
    "# And we fit the training data\n",
    "bag_cancer.fit(X_train, y_train)\n",
    "\n",
    "pred_bag = bag_cancer.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred_bag))\n",
    "print(classification_report(y_test, pred_bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb03c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use 400 trees\n",
    "# By default, the number of features used in each node is the square root of the total number of columns\n",
    "\n",
    "forest_cancer = RandomForestClassifier(n_estimators=400, random_state = 0)\n",
    "\n",
    "# And we fit the training data\n",
    "forest_cancer.fit(X_train, y_train)\n",
    "\n",
    "# Finally look at the results\n",
    "\n",
    "pred_forest = forest_cancer.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred_forest))\n",
    "\n",
    "# As we can see, we improved even more, two misclassified cases are now corrected\n",
    "print(classification_report(y_test, pred_forest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6259d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top\n",
    "pd.Series(data = forest_cancer.feature_importances_, index= X_train.columns).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105b257b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fa59ad",
   "metadata": {},
   "source": [
    "### Task 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba29886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "house_data = pd.read_csv('House_assignment.csv')\n",
    "house_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71d4e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar analysis as in Task 1 can be done focusing on the column price\n",
    "# What you can find that almost all the variables seem to behave similarly, as they either have 0 correlation, \n",
    "# or when they are categorical, mean of price across categories is the same\n",
    "# The only varibale that somewhat reasonable to include is square meter\n",
    "# It is okay if you included others, you cannot really get better results\n",
    "\n",
    "X = house_data[['Area']]\n",
    "y = house_data['Price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ead3a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build base decision tree regression model\n",
    "\n",
    "house_tree = DecisionTreeRegressor(random_state = 42)\n",
    "\n",
    "house_tree.fit(X_train, y_train)\n",
    "house_pred = house_tree.predict(X_test)\n",
    "\n",
    "# The MSE is around 22 million\n",
    "mse_house = MSE(y_test, house_pred)\n",
    "print('MSE:', mse_house)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abcb989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we take the square root of MSE, it is less than 5000\n",
    "# which is not bad a mistake, considering that the average error is approx. 0.1% of the mean price\n",
    "# Using only one variable\n",
    "100 * mse_house**0.5 / house_data.Price.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60820a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
